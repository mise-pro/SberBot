{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:50:47.444571Z",
     "start_time": "2018-05-21T08:50:38.422074Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import data_functions\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster.hierarchical import AgglomerativeClustering\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from gensim import corpora\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "n_clusters=15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:46.950065Z",
     "start_time": "2018-05-21T08:50:47.448579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data, please wait...\n",
      "loading data DONE\n"
     ]
    }
   ],
   "source": [
    "questions_answers, questions_original, answers_original, questions_norm=data_functions.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:47.924662Z",
     "start_time": "2018-05-21T08:51:46.950065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего нормализованных слов: 633179\n",
      "Всего уникальных нормализованных слов: 18488\n",
      "Всего слов в словаре: 3307\n",
      "Отброшены слова повторяемостью меньше 10 и больше 3000\n"
     ]
    }
   ],
   "source": [
    "#сколько слов в словаре\n",
    "def create_dictionary(questions_normalized, low_limit, max_limit):\n",
    "    \"\"\"\n",
    "    Готовим словарь из нормированных вопров    \n",
    "    \"\"\"\n",
    "    words_all_norm=[]\n",
    "    for question in questions_normalized:\n",
    "        for word in question:\n",
    "            words_all_norm.append(word)\n",
    "    print ('Всего нормализованных слов:', len((words_all_norm)))\n",
    "    words_uniq_norm=set(words_all_norm)\n",
    "    print ('Всего уникальных нормализованных слов:',len((words_uniq_norm)))\n",
    "    \n",
    "    words_all_norm_freq = nltk.probability.FreqDist(words_all_norm)\n",
    "    dictionary_words=[]\n",
    "    for word in words_all_norm_freq:\n",
    "        if words_all_norm_freq[word]>low_limit and words_all_norm_freq[word]<max_limit:\n",
    "            dictionary_words.append(word)\n",
    "    print ('Всего слов в словаре:',len(dictionary_words))\n",
    "    print ('Отброшены слова повторяемостью меньше {} и больше {}'.format(low_limit, max_limit))\n",
    "    return dictionary_words\n",
    "\n",
    "dictionary=create_dictionary(questions_norm,10,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:47.940287Z",
     "start_time": "2018-05-21T08:51:47.924662Z"
    }
   },
   "outputs": [],
   "source": [
    "#отобразить перечень слов упорядоченных по повторяемости\n",
    "def get_top_freq_words(words_all_norm, top_positions):\n",
    "    words_all_norm_freq = nltk.probability.FreqDist(words_all_norm)\n",
    "    for w in sorted(words_all_norm_freq, key=words_all_norm_freq.get, reverse=True)[:top_positions]:\n",
    "        print (w, words_all_norm_freq[w])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:51:47.940287Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_all_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3c2f2f8f6456>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_top_freq_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_all_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'words_all_norm' is not defined"
     ]
    }
   ],
   "source": [
    "get_top_freq_words(words_all_norm, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:50:38.437Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([dictionary_words])\n",
    "corpus = [dictionary.doc2bow(text) for text in [x for x in questions_norm]]\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:50:38.441Z"
    }
   },
   "outputs": [],
   "source": [
    "print ('Calculating model...')\n",
    "ldamodel = gensim.models.ldamodel.LdaModel.load('model.gensim')\n",
    "#ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = n_clusters, id2word=dictionary, passes=30)\n",
    "print ('Calculating model... DONE')\n",
    "ldamodel.save('model.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:50:38.445Z"
    }
   },
   "outputs": [],
   "source": [
    "#вывести топики\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "for t, top_words in ldamodel.print_topics(num_topics=10, num_words=10):\n",
    "    print (\"Topic\", t, \":\", top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:50:38.450Z"
    }
   },
   "outputs": [],
   "source": [
    "#определение ближайших \n",
    "current_question='ком звон на представл сотрудник служб социаологическ пользован услуг сбербанк то мне сегодн одн девушк позвон номер 7499xxxxxxx ли удел минут на эт ну соглас на что он зада мне вопрос насчет тог есл за последн месяц мо организац пользова ег услуг ил тог ответ что нет посл чег кто эт мог быт действительн ли провод так опрос кто'\n",
    "\n",
    "current_question = data_functions.clean_text(current_question)\n",
    "current_question_bow = dictionary.doc2bow(current_question)\n",
    "#print(current_question_bow)\n",
    "print(ldamodel.get_document_topics(current_question_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:50:38.454Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#проставляем всем вопросам темы\n",
    "#todo - скорее всего - вычисление еще раз doc2bow для каждого нормированного вопроса - не очень удачная идея\n",
    "question_topics_idx=[]\n",
    "for question in questions_norm[:]:\n",
    "    themes_and_probs_in_question=ldamodel.get_document_topics(dictionary.doc2bow(question))\n",
    "    themes_probs_in_question=[i[1] for i in themes_and_probs_in_question] \n",
    "    question_topics_idx.append(themes_and_probs_in_question[np.argmax(themes_probs_in_question)][0])\n",
    "len(question_topics_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:50:38.457Z"
    }
   },
   "outputs": [],
   "source": [
    "#отразить количество вопрсов в кластерах\n",
    "for theme_num in range(n_clusters):\n",
    "    count=np.array(np.where(np.array(question_topics_idx) == theme_num)).size\n",
    "    count_perc=100.*count/len(question_topics_idx)\n",
    "    print (\"Размер темы \"+ str(theme_num) + ': ' + str(count) + ' %= ' + str(int(count_perc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:50:38.469Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T08:51:48.622756Z",
     "start_time": "2018-05-21T08:50:38.474Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model.gensim')\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display, local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
